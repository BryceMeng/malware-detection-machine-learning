{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Malware detection using machine learning\n",
    "\n",
    "Hao Meng 2020.06 \n",
    "\n",
    "rewirte in notebook in 2021.10\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Recently, to do data mining on financial data features. I have taught myself machine learning, mainly traditional statistical-based machine learning (book: An-Introduction-to-Statistical-Learning) and deep learning based on neural networks (from Hung-yi Lee https://speech.ee.ntu.edu.tw/~ tlkagk/courses.html). I also studied reinforcement learning, but I found some of the concepts difficult to understand, and I think it is one of the most challenging kinds of machine learning to understand. So I haven't actually used it yet.\n",
    "\n",
    "I used to do very intensive research on malware in my last bank and my university days. When I discovered that machine learning is a powerful tool for data analysis, I thought: why can't I use machine learning to detect malware?\n",
    "\n",
    "## Instruction\n",
    "\n",
    "This notebook is mainly based on an existed notebook(https://github.com/dchad/malware-detection), and the data comes from Microsoft Malware Classification Challenge (BIG 2015)(https://www.kaggle.com/c/malware-classification/data), which data quality is high. \n",
    "\n",
    "Why I do my research based on Derek's notebook? Because the workload of analyzing this data from scratch is enormous, I will use his method directly and go through the advantages and disadvantages to speed up the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"/media/xueshan/WD_BLACK/cybersecurity/malaware\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set and test set\n",
    "\n",
    "In kaggle competetion, the test set is non-public. To facilitate the analysis, I split the original training set into a new training set and a test set by 8:2.\n",
    "\n",
    "This is a multiclassification problem. As we can see from the number of samples per class, the samples of class 5 have just 42, far awry from class 1, and we may have to face a prevalent machine learning problem: **class imbalance**. We will process the problem in the last session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Id Counts\n",
      "Class           \n",
      "1           1541\n",
      "2           2478\n",
      "3           2942\n",
      "4            475\n",
      "5             42\n",
      "6            751\n",
      "7            398\n",
      "8           1228\n",
      "9           1013\n",
      "\n",
      "total samples: 10868\n",
      "processing: 1\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "maybe this has been splited",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f42eadf771a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0msplit_train_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f42eadf771a1>\u001b[0m in \u001b[0;36msplit_train_set\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masm_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maybe this has been splited\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masm_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masm_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: maybe this has been splited"
     ]
    }
   ],
   "source": [
    "TRAIN_LABEL_CSV = os.path.join(BASE_DIR, \"trainLabels.csv\")\n",
    "\n",
    "def split_train_set():\n",
    "    df = pd.read_csv(TRAIN_LABEL_CSV)\n",
    "    dfg = df.groupby(by=\"Class\").count()\n",
    "    dfg.columns = [\"Id Counts\"]\n",
    "    print(dfg)\n",
    "    print()\n",
    "    print(\"total samples:\", dfg.sum().values[0])\n",
    "    # fetch 70% in each class to train sets, the left 30% is test sets\n",
    "    for class_level in dfg.index.values:\n",
    "        print(\"processing:\", class_level)\n",
    "        tdf : pd.DataFrame = df.loc[df[\"Class\"] == class_level,]\n",
    "        tdf : pd.DataFrame = tdf.iloc[round(len(tdf)*0.8):]\n",
    "        # just move the 20% to test sets\n",
    "        for row in tdf.itertuples():\n",
    "            asm_file = f\"{row.Id}.asm\"\n",
    "            byte_file = f\"{row.Id}.bytes\"\n",
    "            if not os.path.exists(os.path.join(TRAIN_DIR, asm_file)):\n",
    "                # continue\n",
    "                raise Exception(\"maybe this has been splited\")\n",
    "            shutil.move(os.path.join(TRAIN_DIR, asm_file), os.path.join(TEST_DIR, asm_file))\n",
    "            shutil.move(os.path.join(TRAIN_DIR, byte_file), os.path.join(TEST_DIR, byte_file))\n",
    "\n",
    "        print(class_level, \"done\")\n",
    "        \n",
    "split_train_set()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "Features are an essential part of machine learning. Garbage in, garbage out.I have read some articles, and many methods just extract features from just the head of the PE, which is very unreasonable. Because the PE header contains very limited information, probably only some function information in the input and output tables can be used. The fundamental program execution process is all in the code block of the program, and this part is the core of attention.\n",
    "\n",
    "Tanks to Derek. He used many methods to extract features, and I guess he should be experienced in detecting malware in traditional ways.\n",
    "\n",
    "And I think Microsoft used an excellent way to generate the data. Because when I first thought of extracting the features of the executable, I was worried that maybe analysis of binary code is a vast work. However, Microsoft used one existing and powerful tool named IDA to do this job, and it can export some readable files(.asm), which is very easy to analyze. Yes, we should not build these huge tools over and over again.\n",
    "\n",
    "At last, I choose two type of features. **ASM feature** and **call graph feature**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ASM feature\n",
    "As you can see from the code, there are 4 types of features: registers, opcodes, function calls, and others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating process in file \"extract_asm_feature.py\"\n",
    "# I should not show the whole code in this notebook because the code size it too long and the execution \n",
    "# time migh last several days.\n",
    "\n",
    "keywords = ['Virtual', 'Offset', 'loc', 'Import', 'Imports', 'var', 'Forwarder', 'UINT', 'LONG', 'BOOL', 'WORD',\n",
    "            'BYTES', 'large', 'short', 'dd', 'db', 'dw', 'XREF', 'ptr', 'DATA', 'FUNCTION', 'extrn', 'byte', 'word',\n",
    "            'dword', 'char', 'DWORD', 'stdcall', 'arg', 'locret', 'asc', 'align', 'WinMain', 'unk', 'cookie', 'off',\n",
    "            'nullsub', 'DllEntryPoint', 'System32', 'dll', 'CHUNK', 'BASS', 'HMENU', 'DLL', 'LPWSTR', 'void', 'HRESULT',\n",
    "            'HDC', 'LRESULT', 'HANDLE', 'HWND', 'LPSTR', 'int', 'HLOCAL', 'FARPROC', 'ATOM', 'HMODULE', 'WPARAM',\n",
    "            'HGLOBAL', 'entry', 'rva', 'COLLAPSED', 'config', 'exe', 'Software', 'CurrentVersion', '__imp_', 'INT_PTR',\n",
    "            'UINT_PTR', '---Seperator', 'PCCTL_CONTEXT', '__IMPORT_', 'INTERNET_STATUS_CALLBACK', '.rdata:', '.data:',\n",
    "            '.text:', 'case', 'installdir', 'market', 'microsoft', 'policies', 'proc', 'scrollwindow', 'search', 'trap',\n",
    "            'visualc', '___security_cookie', 'assume', 'callvirtualalloc', 'exportedentry', 'hardware',\n",
    "            'hkey_current_user', 'hkey_local_machine', 'sp-analysisfailed', 'unableto']\n",
    "\n",
    "registers = ['edx', 'esi', 'es', 'fs', 'ds', 'ss', 'gs', 'cs', 'ah', 'al',\n",
    "             'ax', 'bh', 'bl', 'bx', 'ch', 'cl', 'cx', 'dh', 'dl', 'dx',\n",
    "             'eax', 'ebp', 'ebx', 'ecx', 'edi', 'esp']\n",
    "\n",
    "opcodes = ['add', 'al', 'bt', 'call', 'cdq', 'cld', 'cli', 'cmc', 'cmp', 'const', 'cwd', 'daa', 'db'\n",
    "    , 'dd', 'dec', 'dw', 'endp', 'ends', 'faddp', 'fchs', 'fdiv', 'fdivp', 'fdivr', 'fild'\n",
    "    , 'fistp', 'fld', 'fstcw', 'fstcwimul', 'fstp', 'fword', 'fxch', 'imul', 'in', 'inc'\n",
    "    , 'ins', 'int', 'jb', 'je', 'jg', 'jge', 'jl', 'jmp', 'jnb', 'jno', 'jnz', 'jo', 'jz'\n",
    "    , 'lea', 'loope', 'mov', 'movzx', 'mul', 'near', 'neg', 'not', 'or', 'out', 'outs'\n",
    "    , 'pop', 'popf', 'proc', 'push', 'pushf', 'rcl', 'rcr', 'rdtsc', 'rep', 'ret', 'retn'\n",
    "    , 'rol', 'ror', 'sal', 'sar', 'sbb', 'scas', 'setb', 'setle', 'setnle', 'setnz'\n",
    "    , 'setz', 'shl', 'shld', 'shr', 'sidt', 'stc', 'std', 'sti', 'stos', 'sub', 'test'\n",
    "    , 'wait', 'xchg', 'xor']\n",
    "\n",
    "\n",
    "def get_APIS_consts():\n",
    "    \"\"\"\n",
    "    some common functions name\n",
    "    \"\"\"\n",
    "    fapi = open(os.path.join(BASE_DIR, \"APIs.txt\"))\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "    return defined_apis\n",
    "\n",
    "####################################### 4 feature exraction functions ######################################\n",
    "\n",
    "def count_asm_registers(asm_code):\n",
    "    \"\"\"\n",
    "    Count how many times each of the different types of registers are used in this file\n",
    "    :param asm_code:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    registers_values = [0] * len(registers)\n",
    "    for row in asm_code:\n",
    "        parts = row.replace(',', ' ').replace('+', ' ').replace('*', ' ').replace('[', ' ').replace(']', ' ') \\\n",
    "            .replace('-', ' ').split()\n",
    "        # print(parts)\n",
    "        for register in registers:\n",
    "            registers_values[registers.index(register)] += parts.count(register)\n",
    "    return registers_values\n",
    "\n",
    "\n",
    "def count_asm_opcodes(asm_code):\n",
    "    \"\"\"\n",
    "    Count how many times each of the different opcodes are used in this file\n",
    "    :param asm_code:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    opcodes_values = [0] * len(opcodes)\n",
    "    for row in asm_code:\n",
    "        parts = row.split()\n",
    "\n",
    "        for opcode in opcodes:\n",
    "            if opcode in parts:\n",
    "                opcodes_values[opcodes.index(opcode)] += 1\n",
    "                break\n",
    "    return opcodes_values\n",
    "\n",
    "\n",
    "def count_asm_APIs(asm_code, apis):\n",
    "    \"\"\"\n",
    "    Count how many times different functions have been called in this file\n",
    "    \n",
    "    :param asm_code:\n",
    "    :param apis: from get_APIS_consts\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    apis_values = [0] * len(apis)\n",
    "    for row in asm_code:\n",
    "        for i in range(len(apis)):\n",
    "            if apis[i] in row:\n",
    "                apis_values[i] += 1\n",
    "                break\n",
    "    return apis_values\n",
    "\n",
    "\n",
    "def count_asm_misc(asm_code):\n",
    "    \"\"\"\n",
    "    some misc information, like definitions of data, occur times of some string\n",
    "    :param asm_code:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    keywords_values = [0] * len(keywords)\n",
    "    for row in asm_code:\n",
    "        for i in range(len(keywords)):\n",
    "            if keywords[i] in row:\n",
    "                keywords_values[i] += 1\n",
    "                break\n",
    "    return keywords_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### types\n",
    "- Registers\n",
    "\n",
    "The number of times various different registers are called\n",
    "\n",
    "- Opcodes\n",
    "\n",
    "The number of times various different opcodes are called\n",
    "\n",
    "- APIs\n",
    "\n",
    "The number of times various different extern system functions are called. \n",
    "\n",
    "- Misc\n",
    "\n",
    "The number of times various kinds of variables are defined. \n",
    "\n",
    "#### Advantages\n",
    "This way of extracting features is very similar to a very fast and traditional way of virus detection. So subjectively it is predicted that it should be useful. \n",
    "\n",
    "Many years ago, and even now, a speedy way of detecting viruses existed. It was swift, but the downside was that the detection rate against variants was shallow. This way takes some opcodes, registers, and function calls at different offsets of a specific file. Then as long as 80% or 90% of the conditions are met when the detection is performed, the file is determined to be a virus file.\n",
    "\n",
    "THe ASM feature is similar with this traditional style. More advance is that it can collect more data to improve the accuracy of detection. \n",
    "\n",
    "Acctually, there is a behavior-based detection method within the traditional detection method, which is more accurate and slower. We will mention it inside the following category of features.\n",
    "\n",
    "#### Can be improved\n",
    "1. Perhaps feature extraction can be done specifically for export and import tables. \n",
    "2. Perhaps feature extraction can be done specifically for the data segment. \n",
    "\n",
    "#### Problems\n",
    "There is a difficulty that if many files have different feature data, this will lead to a too large sparse matrix of features and generate high-dimensional disasters. So how to better extract and streamline the features is probably the most crucial problem that machine learning needs to focus on for the cyber security field.\n",
    "\n",
    "\n",
    "#### Cracking\n",
    "1. Packing applications like UPX or customized algorithms and even SMC (Self Modifying Code) is possible.\n",
    "2. Replace static import with the dynamic import of system functions. In the Windows system, almost every system function can be replaced with LoadLibrary and GetProcAddress. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. call graph feature\n",
    "\n",
    "First we do data mining from the asm file to generate a function call graph. How to mine?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating process in file \"extract_call_control_feature.py\"\n",
    "# I should not show the whole code in this notebook because the code size it too long and the execution \n",
    "# time migh last several days.\n",
    "\n",
    "call_opcodes = ['call', 'int']\n",
    "call_blocks = ['sub_']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a. function recognization**\n",
    "\n",
    "for example:\n",
    ">.text:00401000\t\t\t\t\t\t       sub_401000      proc near\t       ; CODE XREF: sub_4086B0+6A(up arrow)p\n",
    "\n",
    "The code above is one line assembly code which represent a definition of a function named sub_40100. The CODE XREF shows the address who call the function.\n",
    "\n",
    "**b. function call recognization**\n",
    "\n",
    "for example:\n",
    ">.text:00401026 E8 42 84\t00 00\t\t\t\t\t\t       call    sub_40946D\n",
    "\n",
    "The code above is one line assembly code represents a function named sub_40946D is called. The function whose name starts with sub_ is internal function which code exists inside the file. \n",
    "\n",
    ">.text:004011A5 FF 15 E4\tC0 40 00\t\t\t\t\t       call    ds:VirtualAlloc\n",
    "\n",
    "The code above is one line assembly code represents a system function named VirtualAlloc is called. The function whose name is complete is a system function which code exists in the system file not in the current file.\n",
    "\n",
    "The assembly code starts with \"int\" is a system call, but I didn't find an example.\n",
    "\n",
    "**c. final feature**\n",
    "\n",
    "In the final feature, there is the number of times various functions are called. Yes, there is **no relationship between functions** in the final features. And Derek combined the functions with names starting with **sub_ into few categories**, probably to reduce the dimensionality of the features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages\n",
    "The information on function calls is more comprehensive than ASM features and provides a more detailed picture of a file's characteristics. \n",
    "\n",
    "#### Can be improved\n",
    "The relationship between functions can be characterized by features as well.\n",
    "\n",
    "#### Problems\n",
    "The **crucial information which can represent the relationship between functions** has been lost in the feature. It is indeed too difficult to represent these information. \n",
    "\n",
    "#### Cracking\n",
    "It is the same as the cracking method in ASM feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "1. Try some of the common models in sklean\n",
    "2. Try the mainstream tree model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import log_loss, confusion_matrix, accuracy_score, classification_report, precision_score, \\\n",
    "    recall_score\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "BASE_DIR = \"/media/xueshan/WD_BLACK/cybersecurity/malaware\"\n",
    "# BASE_DIR = \"/home/xueshan/temp/malaware\"\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, \"train\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"test\")\n",
    "\n",
    "DATA_OUT_DIR = os.path.join(BASE_DIR, \"data_out\")\n",
    "\n",
    "TRAIN_FEAFURE_ASM = os.path.join(DATA_OUT_DIR, \"train-malware-features-asm-30percent.csv\")\n",
    "TRAIN_FEAFURE_CALL = os.path.join(DATA_OUT_DIR, \"train-call-graph-features-10percent.csv\")\n",
    "TRAIN_LABEL = os.path.join(DATA_OUT_DIR, \"train-labels.csv\")\n",
    "\n",
    "TEST_FEATURE_ASM = os.path.join(DATA_OUT_DIR, \"test-malware-features-asm-30percent.csv\")\n",
    "TEST_FEATURE_CALL = os.path.join(DATA_OUT_DIR, \"test-call-graph-features-10percent.csv\")\n",
    "TEST_LABEL = os.path.join(DATA_OUT_DIR, \"test-labels.csv\")\n",
    "\n",
    "RELEASE_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    edx   esi  es  ds  ss  cs  ah   al  ax  bh  ...  subst_x  subst_y  unkno  \\\n",
      "0   750   496   3   0   0   0   8  224  49  34  ...      0.0      0.0    1.0   \n",
      "1  1121    24   3   1   4   2   6   22   7   1  ...      0.0      0.0    0.0   \n",
      "2  1493  1900   0   0   0   0   1  398   0   0  ...      0.0      0.0    0.0   \n",
      "3   525     4   0   0   0   0   0    0   0   0  ...      0.0      0.0    0.0   \n",
      "4    23    35   0   0   0   0   0    3   0   0  ...      0.0      0.0    0.0   \n",
      "\n",
      "   unkno_x  unkno_x.1  unkno_x.2  unkno_y  unkno_y.1  unkno_y.2  wpa_hexd  \n",
      "0        1        0.0        0.0        1        0.0        0.0       0.0  \n",
      "1        0        0.0        0.0        0        0.0        0.0       0.0  \n",
      "2        0        0.0        0.0        0        0.0        0.0       0.0  \n",
      "3        0        0.0        0.0        0        0.0        0.0       0.0  \n",
      "4        0        0.0        0.0        0        0.0        0.0       0.0  \n",
      "\n",
      "[5 rows x 2183 columns]\n",
      "                edx           esi           es           ds           ss  \\\n",
      "count   8694.000000   8694.000000  8694.000000  8694.000000  8694.000000   \n",
      "mean    1577.374396   1901.886473     3.951231     1.617437     1.514493   \n",
      "std     4938.101289   5606.492289    45.327510     9.390781     5.654980   \n",
      "min        0.000000      0.000000     0.000000     0.000000     0.000000   \n",
      "25%      117.250000     64.000000     0.000000     0.000000     0.000000   \n",
      "50%      357.000000    212.000000     0.000000     0.000000     0.000000   \n",
      "75%     1023.000000   1058.500000     2.000000     0.000000     0.000000   \n",
      "max    95687.000000  77844.000000  3978.000000   576.000000   149.000000   \n",
      "\n",
      "                cs           ah            al            ax           bh  ...  \\\n",
      "count  8694.000000  8694.000000   8694.000000   8694.000000  8694.000000  ...   \n",
      "mean      0.900736    13.358638    154.048654     50.066828     6.310674  ...   \n",
      "std       3.008813    88.261689    579.987849    274.636795    29.613760  ...   \n",
      "min       0.000000     0.000000      0.000000      0.000000     0.000000  ...   \n",
      "25%       0.000000     0.000000      1.000000      0.000000     0.000000  ...   \n",
      "50%       0.000000     2.000000     24.000000      1.000000     0.000000  ...   \n",
      "75%       0.000000     5.000000    127.000000     21.000000     3.000000  ...   \n",
      "max      86.000000  4798.000000  24462.000000  15935.000000   859.000000  ...   \n",
      "\n",
      "           subst_x      subst_y        unkno      unkno_x    unkno_x.1  \\\n",
      "count  8694.000000  8694.000000  8694.000000  8694.000000  8694.000000   \n",
      "mean      0.036002     0.036002     0.158845     0.210605     0.157005   \n",
      "std       1.638352     1.638352     0.441653     0.497940     0.439702   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max     106.000000   106.000000     3.000000     3.000000     3.000000   \n",
      "\n",
      "         unkno_x.2      unkno_y    unkno_y.1    unkno_y.2     wpa_hexd  \n",
      "count  8694.000000  8694.000000  8694.000000  8694.000000  8694.000000  \n",
      "mean      0.105245     0.210605     0.157005     0.105245     0.063722  \n",
      "std       0.368228     0.497940     0.439702     0.368228     5.941553  \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "max       3.000000     3.000000     3.000000     3.000000   554.000000  \n",
      "\n",
      "[8 rows x 2183 columns]\n",
      "after normalization:\n",
      "        edx       esi        es        ds        ss        cs        ah  \\\n",
      "0 -0.167559 -0.250775 -0.020987 -0.172247 -0.267831 -0.299383 -0.060717   \n",
      "1 -0.092424 -0.334968 -0.020987 -0.065753  0.439551  0.365369 -0.083378   \n",
      "2 -0.017087 -0.000336 -0.087176 -0.172247 -0.267831 -0.299383 -0.140031   \n",
      "3 -0.213125 -0.338535 -0.087176 -0.172247 -0.267831 -0.299383 -0.151361   \n",
      "4 -0.314790 -0.333006 -0.087176 -0.172247 -0.267831 -0.299383 -0.151361   \n",
      "\n",
      "         al        ax        bh  ...   subst_x   subst_y     unkno   unkno_x  \\\n",
      "0  0.120615 -0.003885  0.935069  ... -0.021976 -0.021976  1.904669  1.585412   \n",
      "1 -0.227688 -0.156823 -0.179342  ... -0.021976 -0.021976 -0.359681 -0.422977   \n",
      "2  0.420639 -0.182312 -0.213112  ... -0.021976 -0.021976 -0.359681 -0.422977   \n",
      "3 -0.265622 -0.182312 -0.213112  ... -0.021976 -0.021976 -0.359681 -0.422977   \n",
      "4 -0.260449 -0.182312 -0.213112  ... -0.021976 -0.021976 -0.359681 -0.422977   \n",
      "\n",
      "   unkno_x.1  unkno_x.2   unkno_y  unkno_y.1  unkno_y.2  wpa_hexd  \n",
      "0  -0.357091  -0.285831  1.585412  -0.357091  -0.285831 -0.010725  \n",
      "1  -0.357091  -0.285831 -0.422977  -0.357091  -0.285831 -0.010725  \n",
      "2  -0.357091  -0.285831 -0.422977  -0.357091  -0.285831 -0.010725  \n",
      "3  -0.357091  -0.285831 -0.422977  -0.357091  -0.285831 -0.010725  \n",
      "4  -0.357091  -0.285831 -0.422977  -0.357091  -0.285831 -0.010725  \n",
      "\n",
      "[5 rows x 2183 columns]\n",
      "                edx           esi            es            ds            ss  \\\n",
      "count  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03   \n",
      "mean  -2.207293e-17 -5.910590e-17  5.402824e-16 -1.891619e-16  4.474860e-16   \n",
      "std    1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00   \n",
      "min   -3.194477e-01 -3.392488e-01 -8.717571e-02 -1.722466e-01 -2.678312e-01   \n",
      "25%   -2.957024e-01 -3.278328e-01 -8.717571e-02 -1.722466e-01 -2.678312e-01   \n",
      "50%   -2.471486e-01 -3.014333e-01 -8.717571e-02 -1.722466e-01 -2.678312e-01   \n",
      "75%   -1.122711e-01 -1.504390e-01 -4.304986e-02 -1.722466e-01 -2.678312e-01   \n",
      "max    1.905895e+01  1.354617e+01  8.767915e+01  6.116803e+01  2.608214e+01   \n",
      "\n",
      "                 cs            ah            al            ax            bh  \\\n",
      "count  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03   \n",
      "mean   7.183886e-16  1.890916e-16  1.238992e-16 -1.087844e-16  9.481718e-17   \n",
      "std    1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00   \n",
      "min   -2.993832e-01 -1.513613e-01 -2.656220e-01 -1.823125e-01 -2.131116e-01   \n",
      "25%   -2.993832e-01 -1.513613e-01 -2.638977e-01 -1.823125e-01 -2.131116e-01   \n",
      "50%   -2.993832e-01 -1.287001e-01 -2.242394e-01 -1.786711e-01 -2.131116e-01   \n",
      "75%   -2.993832e-01 -9.470835e-02 -4.663927e-02 -1.058434e-01 -1.118016e-01   \n",
      "max    2.828496e+01  5.421284e+01  4.191355e+01  5.784311e+01  2.879534e+01   \n",
      "\n",
      "       ...       subst_x       subst_y         unkno       unkno_x  \\\n",
      "count  ...  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03   \n",
      "mean   ...  1.858710e-15  1.858710e-15  8.092599e-16 -2.417359e-17   \n",
      "std    ...  1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00   \n",
      "min    ... -2.197569e-02 -2.197569e-02 -3.596812e-01 -4.229766e-01   \n",
      "25%    ... -2.197569e-02 -2.197569e-02 -3.596812e-01 -4.229766e-01   \n",
      "50%    ... -2.197569e-02 -2.197569e-02 -3.596812e-01 -4.229766e-01   \n",
      "75%    ... -2.197569e-02 -2.197569e-02 -3.596812e-01 -4.229766e-01   \n",
      "max    ...  6.468090e+01  6.468090e+01  6.433370e+00  5.602188e+00   \n",
      "\n",
      "          unkno_x.1     unkno_x.2       unkno_y     unkno_y.1     unkno_y.2  \\\n",
      "count  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03  8.694000e+03   \n",
      "mean  -3.501659e-16  2.039368e-17 -2.417359e-17 -3.501659e-16  2.039368e-17   \n",
      "std    1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00  1.000058e+00   \n",
      "min   -3.570913e-01 -2.858311e-01 -4.229766e-01 -3.570913e-01 -2.858311e-01   \n",
      "25%   -3.570913e-01 -2.858311e-01 -4.229766e-01 -3.570913e-01 -2.858311e-01   \n",
      "50%   -3.570913e-01 -2.858311e-01 -4.229766e-01 -3.570913e-01 -2.858311e-01   \n",
      "75%   -3.570913e-01 -2.858311e-01 -4.229766e-01 -3.570913e-01 -2.858311e-01   \n",
      "max    6.466099e+00  7.861761e+00  5.602188e+00  6.466099e+00  7.861761e+00   \n",
      "\n",
      "           wpa_hexd  \n",
      "count  8.694000e+03  \n",
      "mean  -5.414656e-16  \n",
      "std    1.000058e+00  \n",
      "min   -1.072544e-02  \n",
      "25%   -1.072544e-02  \n",
      "50%   -1.072544e-02  \n",
      "75%   -1.072544e-02  \n",
      "max    9.323626e+01  \n",
      "\n",
      "[8 rows x 2183 columns]\n",
      "train features: (8694, 2183)\n",
      "test features: (2174, 2183)\n",
      "exception data number: 0\n"
     ]
    }
   ],
   "source": [
    "def normalize_feature(df_train, df_test):\n",
    "    \"\"\"\n",
    "    normalize data to the data which mean is 0 and stdev is 1\n",
    "    :param df_train:\n",
    "    :param df_test:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    std = StandardScaler()\n",
    "    for i in range(df_train.shape[1]):\n",
    "        #  Use the mean and std of train data to transfrom test data\n",
    "        std.fit(df_train.iloc[:, i].values.reshape(-1, 1))\n",
    "        df_train.iloc[:, i] = std.transform(df_train.iloc[:, i].values.reshape(-1, 1)).flatten()\n",
    "        df_test.iloc[:, i] = std.transform(df_test.iloc[:, i].values.reshape(-1, 1)).flatten()\n",
    "        \n",
    "def get_data_set():\n",
    "    # read train\n",
    "    df_train_asm = pd.read_csv(TRAIN_FEAFURE_ASM)\n",
    "    df_train_call = pd.read_csv(TRAIN_FEAFURE_CALL)\n",
    "    df_train_label = pd.read_csv(TRAIN_LABEL)\n",
    "\n",
    "    df_test_asm = pd.read_csv(TEST_FEATURE_ASM)\n",
    "    df_test_call = pd.read_csv(TEST_FEATURE_CALL)\n",
    "    df_test_label = pd.read_csv(TEST_LABEL)\n",
    "\n",
    "    assert np.all(df_test_asm.index == df_test_call.index) and np.all(df_train_call.index == df_train_label.index)\n",
    "\n",
    "    df_train_features = pd.concat([df_train_asm.iloc[:, 1:], df_train_call.iloc[:, 1:]], axis=1)\n",
    "    df_test_features = pd.concat([df_test_asm.iloc[:, 1:], df_test_call.iloc[:, 1:]], axis=1)\n",
    "\n",
    "    print(df_train_features.head())\n",
    "    print(df_train_features.describe())\n",
    "    \n",
    "    # In fact, it is not necessary to normalize the features in tree-based models\n",
    "    # However, for the sake of standardization, we still normalize the data\n",
    "    normalize_feature(df_train_features, df_test_features)\n",
    "    print(\"after normalization:\")\n",
    "\n",
    "    print(df_train_features.head())\n",
    "    print(df_train_features.describe())\n",
    "\n",
    "    print(\"train features:\", df_train_features.shape)\n",
    "    print(\"test features:\", df_test_features.shape)\n",
    "    print(\"exception data number:\", df_train_features.isnull().sum().sum())\n",
    "\n",
    "    X_train = df_train_features\n",
    "    y_train = df_train_label.iloc[:, 1].values\n",
    "    X_test = df_test_features\n",
    "    y_test = df_test_label.iloc[:, 1].values\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_data_set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some common models in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(X, y, clf):\n",
    "    # Construct a kfolds object\n",
    "    kf = KFold(10 if RELEASE_MODE else 3, shuffle=True)\n",
    "    y_prob = np.zeros((len(y), 9))\n",
    "    y_pred = np.zeros(len(y))\n",
    "\n",
    "    # Iterate through folds\n",
    "    for idx, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        print(\"iter:\", idx + 1, \"train len:\", len(train_index), \"test len:\", len(test_index))\n",
    "        X_train = X.loc[train_index, :]\n",
    "        X_test = X.loc[test_index, :]\n",
    "        y_train = y[train_index]\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        y_pred[test_index] = clf.predict(X_test)\n",
    "\n",
    "    return y_prob, y_pred\n",
    "\n",
    "def sklearn_model_selection():\n",
    "    names = [\"Nearest Neighbors\",\n",
    "             # \"Linear SVM\",\n",
    "             # \"RBF SVM\",\n",
    "             \"Decision Tree\",\n",
    "             \"Random Forest\",\n",
    "             \"AdaBoost\",\n",
    "             \"Naive Bayes\",\n",
    "             # \"Linear Discriminant Analysis\",\n",
    "             # \"Quadratic Discriminant Analysis\",\n",
    "             \"Extra Trees\"]\n",
    "\n",
    "    classifiers = [\n",
    "        KNeighborsClassifier(),\n",
    "        # SVC(kernel=\"linear\", C=0.025, probability=True),\n",
    "        # SVC(gamma=2, C=1, probability=True),\n",
    "        DecisionTreeClassifier(max_depth=5),\n",
    "        RandomForestClassifier(max_depth=5, n_estimators=1000),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "        # LinearDiscriminantAnalysis(),\n",
    "        # QuadraticDiscriminantAnalysis(),\n",
    "        ExtraTreesClassifier(n_estimators=1000)]\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        print(name)\n",
    "        prob, pred = run_cv(X_train, y_train, clf)\n",
    "        # print(\"logloss: %.3f\" % log_loss(y_train, prob))\n",
    "        # print(\"train score = {:.4f}\".format(accuracy_score(y_train, pred)))\n",
    "        # cm = confusion_matrix(y_train, pred)\n",
    "        # print(cm)\n",
    "\n",
    "        print(\"test set:\")\n",
    "\n",
    "        # print result in test set\n",
    "        test_pred = clf.predict(X_test)\n",
    "        print(\"Test accuracy: {:.4f}\".format(accuracy_score(y_test, test_pred)))\n",
    "        print(\"Test precision: {:.4f}\".format(precision_score(y_test, test_pred, average='weighted')))\n",
    "        print(\"Test recall: {:.4f}\".format(recall_score(y_test, test_pred, average='weighted')))\n",
    "        test_cm = confusion_matrix(y_test, test_pred)\n",
    "        print(test_cm)\n",
    "        print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest Neighbors\n",
      "iter: 1 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 2 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 3 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9788\n",
      "Test precision: 0.9790\n",
      "Test recall: 0.9788\n",
      "[[293   2   5   4   0   2   1   1   0]\n",
      " [  1 495   0   0   0   0   0   0   0]\n",
      " [  0   0 588   0   0   0   0   0   0]\n",
      " [  0   0   1  93   0   1   0   0   0]\n",
      " [  1   0   2   0   3   2   0   0   0]\n",
      " [  0   0   5   0   0 145   0   0   0]\n",
      " [  0   0   0   1   0   0  79   0   0]\n",
      " [  5   0   0   2   0   0   0 232   7]\n",
      " [  0   0   0   0   0   0   0   3 200]]\n",
      "------------------------------\n",
      "Decision Tree\n",
      "iter: 1 train len: 5796 test len: 2898\n",
      "iter: 2 train len: 5796 test len: 2898\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "test set:\n",
      "Test accuracy: 0.9416\n",
      "Test precision: 0.9508\n",
      "Test recall: 0.9416\n",
      "[[290   0   0   3   1  14   0   0   0]\n",
      " [  0 495   0   1   0   0   0   0   0]\n",
      " [  0   0 587   0   1   0   0   0   0]\n",
      " [  1   0   0  93   0   1   0   0   0]\n",
      " [  2   0   0   0   1   4   1   0   0]\n",
      " [  6   0   1   3   0 139   1   0   0]\n",
      " [  1   0   0  68   0   0  11   0   0]\n",
      " [  1   0   1   0   0   0   0 231  13]\n",
      " [  0   0   0   0   0   0   0   3 200]]\n",
      "------------------------------\n",
      "Random Forest\n",
      "iter: 1 train len: 5796 test len: 2898\n",
      "iter: 2 train len: 5796 test len: 2898\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "test set:\n",
      "Test accuracy: 0.9811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision: 0.9780\n",
      "Test recall: 0.9811\n",
      "[[307   0   0   0   0   1   0   0   0]\n",
      " [  0 496   0   0   0   0   0   0   0]\n",
      " [  1   0 587   0   0   0   0   0   0]\n",
      " [  0   0   1  94   0   0   0   0   0]\n",
      " [  2   0   0   0   0   6   0   0   0]\n",
      " [ 11   0   0   0   0 138   1   0   0]\n",
      " [  0   0   0   3   0  12  65   0   0]\n",
      " [  0   0   0   0   0   0   0 245   1]\n",
      " [  0   0   0   0   0   0   0   2 201]]\n",
      "------------------------------\n",
      "AdaBoost\n",
      "iter: 1 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 2 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 3 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n",
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/base.py:442: UserWarning: X does not have valid feature names, but AdaBoostClassifier was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4816\n",
      "Test precision: 0.3254\n",
      "Test recall: 0.4816\n",
      "[[308   0   0   0   0   0   0   0   0]\n",
      " [  1 495   0   0   0   0   0   0   0]\n",
      " [588   0   0   0   0   0   0   0   0]\n",
      " [ 95   0   0   0   0   0   0   0   0]\n",
      " [  8   0   0   0   0   0   0   0   0]\n",
      " [150   0   0   0   0   0   0   0   0]\n",
      " [ 80   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 244   2]\n",
      " [  0   0   0   0   0   0   0 203   0]]\n",
      "------------------------------\n",
      "Naive Bayes\n",
      "iter: 1 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 2 train len: 5796 test len: 2898\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "test set:\n",
      "Test accuracy: 0.9747\n",
      "Test precision: 0.9754\n",
      "Test recall: 0.9747\n",
      "[[302   1   0   0   1   0   4   0   0]\n",
      " [  0 496   0   0   0   0   0   0   0]\n",
      " [  0   0 581   0   0   3   4   0   0]\n",
      " [  3   0   0  91   0   1   0   0   0]\n",
      " [  0   1   0   0   6   1   0   0   0]\n",
      " [  5   0   0   0   1 144   0   0   0]\n",
      " [  1   0   0   3   3   6  67   0   0]\n",
      " [  5   1   0   0   0   0   0 233   7]\n",
      " [  0   1   0   0   0   0   0   3 199]]\n",
      "------------------------------\n",
      "Extra Trees\n",
      "iter: 1 train len: 5796 test len: 2898\n",
      "iter: 2 train len: 5796 test len: 2898\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "test set:\n",
      "Test accuracy: 0.9982\n",
      "Test precision: 0.9982\n",
      "Test recall: 0.9982\n",
      "[[308   0   0   0   0   0   0   0   0]\n",
      " [  0 496   0   0   0   0   0   0   0]\n",
      " [  0   0 588   0   0   0   0   0   0]\n",
      " [  1   0   0  94   0   0   0   0   0]\n",
      " [  0   0   0   0   6   2   0   0   0]\n",
      " [  0   0   0   0   0 150   0   0   0]\n",
      " [  0   0   0   0   0   0  80   0   0]\n",
      " [  0   0   0   0   0   0   0 246   0]\n",
      " [  0   0   0   0   0   0   0   1 202]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "sklearn_model_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model ExtraTreesClassifier had a better performance. This model is not the same as a random forest when sampling each tree. RandomForest use bagging, while ExtraTrees use total samples in every tree. In this case, it is obvious that ExtraTrees is better.\n",
    "\n",
    "\n",
    "**References**\n",
    "\n",
    "1\n",
    "P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”, Machine Learning, 63(1), 3-42, 2006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train with each of the 3 models\n",
    "\n",
    "### ExtraTrees\n",
    "\n",
    "As mentioned above, the class imbalance problem should be resolved in model running. We can adjust class weight in the running model using the parameter class_weight. Nowadays, the Machine Learning software is so easy to use as we only set the class_weight to **\"balanced\"** can resolve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exe_ExtraTreesClassifier():\n",
    "\n",
    "    clf1: ExtraTreesClassifier = ExtraTreesClassifier(n_estimators=1000, max_features=None, min_samples_leaf=1,\n",
    "                                                      min_samples_split=9, n_jobs=14,\n",
    "                                                      class_weight=\"balanced\",\n",
    "                                                      criterion='gini')\n",
    "    p1, pred1 = run_cv(X_train, y_train, clf1)\n",
    "    print(\"logloss = {:.4f}\".format(log_loss(y_train, p1)))\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy_score(y_train, pred1)))\n",
    "    print(\"Precision: {:.4f}\".format(precision_score(y_train, pred1, average='weighted')))\n",
    "    print(\"Recall: {:.4f}\".format(recall_score(y_train, pred1, average='weighted')))    \n",
    "    cm = confusion_matrix(y_train, pred1)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"test set:\")\n",
    "\n",
    "    # print result in test set\n",
    "    test_pred = clf1.predict(X_test)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy_score(y_test, test_pred)))\n",
    "    print(\"Test precision: {:.4f}\".format(precision_score(y_test, test_pred, average='weighted')))\n",
    "    print(\"Test recall: {:.4f}\".format(recall_score(y_test, test_pred, average='weighted')))    \n",
    "    test_cm = confusion_matrix(y_test, test_pred)\n",
    "    print(test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 train len: 5796 test len: 2898\n",
      "iter: 2 train len: 5796 test len: 2898\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "logloss = 0.0156\n",
      "Accuracy: 0.9972\n",
      "Precision: 0.9972\n",
      "Recall: 0.9972\n",
      "[[1233    0    0    0    0    0    0    0    0]\n",
      " [   1 1978    2    0    0    0    0    1    0]\n",
      " [   0    0 2352    0    0    0    2    0    0]\n",
      " [   0    0    0  380    0    0    0    0    0]\n",
      " [   3    0    0    0   30    1    0    0    0]\n",
      " [   3    0    0    0    0  598    0    0    0]\n",
      " [   0    0    0    0    0    0  318    0    0]\n",
      " [   0    0    0    0    0    0    0  977    5]\n",
      " [   0    0    0    0    0    0    0    6  804]]\n",
      "test set:\n",
      "Test accuracy: 0.9959\n",
      "Test precision: 0.9959\n",
      "Test recall: 0.9959\n",
      "[[306   0   0   0   0   2   0   0   0]\n",
      " [  0 495   0   0   0   0   0   1   0]\n",
      " [  0   0 588   0   0   0   0   0   0]\n",
      " [  2   0   0  93   0   0   0   0   0]\n",
      " [  1   0   0   0   7   0   0   0   0]\n",
      " [  0   0   0   0   1 149   0   0   0]\n",
      " [  0   0   0   0   0   0  80   0   0]\n",
      " [  0   0   0   0   0   0   0 246   0]\n",
      " [  0   0   0   0   0   0   0   2 201]]\n"
     ]
    }
   ],
   "source": [
    "exe_ExtraTreesClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "def exe_xgboost():\n",
    "\n",
    "    cols = X_train.columns.values\n",
    "    rel = {}\n",
    "    for col in cols:\n",
    "        if col not in rel:\n",
    "            rel[col] = 1\n",
    "        else:\n",
    "            rel[col] += 1\n",
    "            print(col, \":\", rel[col])\n",
    "\n",
    "    new_cols = range(len(cols))\n",
    "    X_train.columns = new_cols\n",
    "    X_test.columns = new_cols\n",
    "\n",
    "    assert len(X_train.columns) == len(set(X_train.columns))\n",
    "\n",
    "    xgclf = xgb.XGBClassifier(n_estimators=1000, objective=\"multi:softmax\", tree_method='gpu_hist', gpu_id=0)\n",
    "    prob, pred = run_cv(X_train, y_train, xgclf)\n",
    "    print(\"logloss: {:.4f}\".format(log_loss(y_train, prob)))\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy_score(y_train, pred)))\n",
    "    print(\"Precision: {:.4f}\".format(precision_score(y_train, pred, average='weighted')))\n",
    "    print(\"Recall: {:.4f}\".format(recall_score(y_train, pred, average='weighted')))\n",
    "    cm = confusion_matrix(y_train, pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"test set:\")\n",
    "\n",
    "    # print result in test set\n",
    "    test_pred = xgclf.predict(X_test)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy_score(y_test, test_pred)))\n",
    "    print(\"Test precision: {:.4f}\".format(precision_score(y_test, test_pred, average='weighted')))\n",
    "    print(\"Test recall: {:.4f}\".format(recall_score(y_test, test_pred, average='weighted')))\n",
    "    test_cm = confusion_matrix(y_test, test_pred)\n",
    "    print(test_cm)\n",
    "    print(\"-\" * 30)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eax : 2\n",
      "ebx : 2\n",
      "ecx : 2\n",
      "edi : 2\n",
      "edx : 2\n",
      "esp : 2\n",
      "iter: 1 train len: 5796 test len: 2898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:59:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "iter: 2 train len: 5796 test len: 2898\n",
      "[23:00:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "[23:00:36] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "logloss: 0.0130\n",
      "Accuracy: 0.9968\n",
      "Precision: 0.9968\n",
      "Recall: 0.9968\n",
      "[[1232    0    0    0    0    1    0    0    0]\n",
      " [   1 1978    0    1    0    1    1    0    0]\n",
      " [   0    0 2353    0    0    0    1    0    0]\n",
      " [   0    0    0  380    0    0    0    0    0]\n",
      " [   2    0    1    0   28    3    0    0    0]\n",
      " [   4    0    0    0    1  596    0    0    0]\n",
      " [   0    0    0    0    0    0  318    0    0]\n",
      " [   0    0    0    0    0    0    0  977    5]\n",
      " [   0    1    0    0    0    0    0    5  804]]\n",
      "test set:\n",
      "Test accuracy: 0.9968\n",
      "Test precision: 0.9968\n",
      "Test recall: 0.9968\n",
      "[[308   0   0   0   0   0   0   0   0]\n",
      " [  1 495   0   0   0   0   0   0   0]\n",
      " [  0   0 588   0   0   0   0   0   0]\n",
      " [  0   0   0  94   0   1   0   0   0]\n",
      " [  1   0   0   0   7   0   0   0   0]\n",
      " [  0   0   1   0   0 149   0   0   0]\n",
      " [  0   0   0   0   0   0  80   0   0]\n",
      " [  0   0   0   0   0   0   0 245   1]\n",
      " [  0   0   0   0   0   0   0   2 201]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "exe_xgboost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueshan/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "\n",
    "def exe_lightgbm():\n",
    "    \n",
    "    assert len(X_train.columns) == len(set(X_train.columns))\n",
    "\n",
    "    gbm = LGBMClassifier(\"dart\", n_estimators=1000, class_weight=\"balanced\", device='gpu')\n",
    "    prob, pred = run_cv(X_train, y_train, gbm)\n",
    "    print(\"logloss: {:.4f}\".format(log_loss(y_train, prob)))\n",
    "    print(\"Accuracy: {:.4f}\".format(accuracy_score(y_train, pred)))\n",
    "    print(\"Precision: {:.4f}\".format(precision_score(y_train, pred, average='weighted')))\n",
    "    print(\"Recall: {:.4f}\".format(recall_score(y_train, pred, average='weighted')))\n",
    "    cm = confusion_matrix(y_train, pred)\n",
    "    print(cm)\n",
    "\n",
    "    print(\"test set:\")\n",
    "\n",
    "    # print result in test set\n",
    "    test_pred = gbm.predict(X_test)\n",
    "    print(\"Test accuracy: {:.4f}\".format(accuracy_score(y_test, test_pred)))\n",
    "    print(\"Test precision: {:.4f}\".format(precision_score(y_test, test_pred, average='weighted')))\n",
    "    print(\"Test recall: {:.4f}\".format(recall_score(y_test, test_pred, average='weighted')))\n",
    "    test_cm = confusion_matrix(y_test, test_pred)\n",
    "    print(test_cm)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 train len: 5796 test len: 2898\n",
      "iter: 2 train len: 5796 test len: 2898\n",
      "iter: 3 train len: 5796 test len: 2898\n",
      "logloss: 0.0127\n",
      "Accuracy: 0.9978\n",
      "Precision: 0.9978\n",
      "Recall: 0.9978\n",
      "[[1233    0    0    0    0    0    0    0    0]\n",
      " [   1 1980    1    0    0    0    0    0    0]\n",
      " [   0    0 2353    0    0    0    1    0    0]\n",
      " [   0    0    0  380    0    0    0    0    0]\n",
      " [   0    0    1    0   32    1    0    0    0]\n",
      " [   4    0    0    0    1  596    0    0    0]\n",
      " [   0    0    0    0    0    0  318    0    0]\n",
      " [   0    0    0    0    0    0    0  979    3]\n",
      " [   0    1    0    0    0    0    0    5  804]]\n",
      "test set:\n",
      "Test accuracy: 0.9977\n",
      "Test precision: 0.9977\n",
      "Test recall: 0.9977\n",
      "[[308   0   0   0   0   0   0   0   0]\n",
      " [  0 495   0   0   0   0   1   0   0]\n",
      " [  0   0 588   0   0   0   0   0   0]\n",
      " [  0   0   0  95   0   0   0   0   0]\n",
      " [  1   0   0   0   7   0   0   0   0]\n",
      " [  0   0   0   0   0 150   0   0   0]\n",
      " [  0   0   0   1   0   0  79   0   0]\n",
      " [  0   0   0   0   0   0   0 246   0]\n",
      " [  0   0   0   0   0   0   0   2 201]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "exe_lightgbm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "\n",
    "As we can see, the three models all have a excellent performance. Even when the feature extraction stage is not done in great detail, the accuracy of these mainstream models can be greater than 99%, and can machine learning is very powerful for malware identification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
