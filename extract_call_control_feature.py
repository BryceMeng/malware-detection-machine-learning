"""
各自的调用关系

vertext:
某个函数称之为节点
.text:00401000						       sub_401000      proc near	       ; CODE XREF: sub_4086B0+6A(up arrow)p
sub_401000 就是一个函数，40100是函数的地址

edge:
调用其他的函数，包括自己写的函数和系统函数(有函数名)
.text:00401026 E8 42 84	00 00						       call    sub_40946D
.text:004011A5 FF 15 E4	C0 40 00					       call    ds:VirtualAlloc

会把每一个函数里面的所有函数调用(call 和 int 系统中断调用)都记录下来

generate_column_names
这一步会把上一步生成的文件当输入源，但是去掉了函数间的逻辑关系，只保留了所有用到的函数名
然后只会记录API.txt里面没有的函数名做为feature记录下来，里面有一点特别有意思，其实每个文件里面的sub_是不一样的，他居然还把sub_1这种形式进行缩减，这是肯定有问题的

generate_function_counts
操作和 generate_column_names 很类似
通过上一步生成的 all-reduced-function-column-names.csv 获取 columns，
然后统计这些函数出现的次数，有一个迷操作就是把sub_4或者sub_5统计成一类，这是严重不合理的



"""
import numpy as np
import pandas as pd
import libs.graph as gra  # http://www.python-course.eu/graphs_python.php
import os
from csv import writer
from multiprocessing import Pool

BASE_DIR = "/media/xueshan/WD_BLACK/cybersecurity/malaware"
# BASE_DIR = "/home/xueshan/temp/malaware"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
TEST_DIR = os.path.join(BASE_DIR, "test")

DATA_OUT_DIR = os.path.join(BASE_DIR, "data_out")

EXAMPLE_EXAMPLE_DIR = os.path.join(BASE_DIR, "dataSample")
EXAMPLE_ASM_FILE = os.path.join(BASE_DIR, "train", "7rneDC6Ng10vEPuicRWV.asm")
EXAMPLE_BYTES_FILE = os.path.join(BASE_DIR, "dataSample", "0ACDbR5M3ZhBJajygTuf.bytes")

SAPMLE_DIR = TRAIN_DIR

call_opcodes = ['call', 'int']
call_blocks = ['sub_']


def construct_call_graph(lines):
    vertex = '.program_entry_point'  # this is the root node, corresponds to the program entry point not C main().
    vertex_count = 1
    edge_count = 0
    cfgraph = gra.Graph()
    cfgraph.add_vertex(vertex)

    for row in lines:
        row = row.rstrip('\r\n')  # get rid of newlines they are annoying.
        if ';' in row:
            row = row.split(';')[0]  # get rid of comments they are annoying.
            # print(row)

        # get rid of all these things they are annoying.
        row = row.replace('short', '').replace('ds:', ' ')
        row = row.replace('dword', '').replace('near', '')
        row = row.replace('ptr', '').replace(':', ' ').replace(',', ' ')  # .replace('??',' ')
        row = row.replace('@', '').replace('?', '')
        parts = row.split()  # tokenize code line

        if (len(parts) < 4):  # this is just a comment line
            continue

        if (parts[3] == 'endp'):  # ignore subroutine end labels
            continue

        # check for subroutines and block labels
        # block and subroutine labels are always after the .text HHHHHHHH relative address
        for block in call_blocks:
            token = parts[2]
            idx = token.find(block)
            if ((idx == 0) or (parts[3] == 'proc')):
                # add new vertex to the graph, we are now in a new subroutine
                vertex = token
                cfgraph.add_vertex(vertex)
                # print("Vertex: " + vertex, parts)
                vertex_count += 1
                break

        # now check for edge opcode
        for opcode in call_opcodes:  # check the line for a new edge
            if opcode in parts:
                # Extract desination address/function name/interrupt number as the directed edge.
                idx = parts.index(opcode)
                edge_count += 1
                if ((idx + 1) < len(parts)):  # in a few ASM files there is no operand, disassembly error?
                    next_vertex = parts[idx + 1]
                else:
                    next_vertex = "none"
                cfgraph.add_edge(vertex, next_vertex)
                # print("Edge: " + vertex + " " + parts[idx], parts)
                break

    # print("Vertex Count: {:d}".format(vertex_count))

    return cfgraph


def extract_call_graphs(tfiles):
    asm_files = [i for i in tfiles if '.asm' in i]
    ftot = len(asm_files)

    pid = os.getpid()
    print('Process id:', pid)

    if SAPMLE_DIR == TRAIN_DIR:
        feature_file = os.path.join(DATA_OUT_DIR,
                                    str(pid) + '-malware-call-graph-features_train.csv')  # Windows API, symbols, registers, opcodes, etc...
    elif SAPMLE_DIR == TEST_DIR:
        feature_file = os.path.join(DATA_OUT_DIR,
                                    str(pid) + '-malware-call-graph-features_test.csv')  # Windows API, symbols, registers, opcodes, etc...
    else:
        raise Exception("unkown sample dir", SAPMLE_DIR)
    print('Graph Feature file:', feature_file)

    graph_lines = []
    graph_features = []
    graph_file = open(os.path.join(DATA_OUT_DIR, str(pid) + '-malware-call-graphs.gv'),
                      'w')  # write as a graphviz DOT format file
    with open(feature_file, 'w') as f:
        # write the column names for the csv file
        fw = writer(f)
        # colnames = ['filename','vertex_count','edge_count','delta_max','density','diameter']
        colnames = ['filename', 'vertex_count', 'edge_count', 'delta_max', 'density']
        fw.writerow(colnames)

        # Now iterate through the file list and extract the call graph from each file.
        for idx, fname in enumerate(asm_files):
            fasm = open(os.path.join(SAPMLE_DIR, fname), 'r', errors='ignore')

            lines = fasm.readlines()

            call_graph = construct_call_graph(lines)
            cgvc = call_graph.n_vertices()
            cgec = call_graph.n_edges()
            cgdm = call_graph.delta_max()
            cgde = call_graph.density()
            # cdia = call_graph.diameter() this is constantly problematic !!!
            graph_features.append([fname[:fname.find('.asm')]] + [cgvc, cgec, cgdm, cgde])
            call_graph.set_graph_name(fname[:fname.find('.asm')])
            # graph_lines.append(call_graph.to_str('multinoleaf'))
            graph_lines.append(call_graph.to_str('singlenoleaf'))

            del (call_graph)  # for some reason new graphs get appended to the previous graphs if not deleted???

            # Print progress
            if (idx + 1) % 10 == 0:
                print(pid, idx + 1, 'of', ftot, 'files processed.')
                fw.writerows(graph_features)
                graph_file.writelines(graph_lines)
                graph_features = []
                graph_lines = []

        # Write remaining files
        if len(graph_lines) > 0:
            fw.writerows(graph_features)
            graph_file.writelines(graph_lines)
            graph_features = []
            graph_lines = []

    graph_file.close()

    print('Process id: {:d} finished.'.format(pid))


def main_work():
    extract_call_graphs(["6cdHfleOSjY0BJxUZNMP.asm"])
    exit(0)

    n_jobs = 16
    arguments = []

    file_lst = [f for f in os.listdir(SAPMLE_DIR) if f.endswith(".asm")]
    print("total samples:", len(file_lst))
    segments = np.linspace(0, len(file_lst), n_jobs + 1).astype(int)
    for idx, i in enumerate(range(n_jobs)):
        arguments.append(file_lst[segments[i]:segments[i + 1]])
        print(idx, ":", segments[i], "->", segments[i + 1], len(arguments[-1]))

    p = Pool(n_jobs)
    p.map(extract_call_graphs, arguments)


def concact_result():
    if SAPMLE_DIR == TRAIN_DIR:
        key = "train"
    elif SAPMLE_DIR == TEST_DIR:
        key = "test"
    else:
        raise Exception("unkown sample dir " + SAPMLE_DIR)

    rel_files = [f for f in os.listdir(DATA_OUT_DIR) if f.endswith(f"-malware-call-graph-features_{key}.csv")]

    df: pd.DataFrame = None
    for rel_file in rel_files:
        tdf = pd.read_csv(os.path.join(DATA_OUT_DIR, rel_file))
        if len(tdf) > 0:
            if df is None:
                df = tdf
            else:
                df = pd.concat([df, tdf], axis=0)

    # print(df)
    df.to_csv(os.path.join(DATA_OUT_DIR, f"{key}-malware-call-graph-features.csv"))


def generate_column_names(call_graph_file):
    counter = 0
    column_names = ['filename']
    graph_names = []
    graph_name = "none"
    graph_functions = {}

    fapi = open(os.path.join(BASE_DIR, "APIs.txt"))
    defined_apis = fapi.readlines()
    defined_apis = defined_apis[0].split(',')
    fapi.close()

    pid = os.getpid()
    print('Process id:', pid)
    column_names_file = os.path.join(DATA_OUT_DIR, str(pid) + '-reduced-column-names.csv')
    print('Column names file: {:s}'.format(column_names_file))
    graph_names_file = os.path.join(DATA_OUT_DIR, str(pid) + '-graph-names.csv')
    print('Graph names file: {:s}'.format(graph_names_file))

    with open(os.path.join(DATA_OUT_DIR, call_graph_file), 'r', errors='ignore') as cfg:
        print("Starting graph file: {:s}".format(call_graph_file))
        for line in cfg:
            line = line.rstrip('\r\n')  # get rid of newlines they are annoying.
            # get rid of all these things they are annoying.
            line = line.replace(',', ' ').replace('[', ' ').replace(']', ' ').replace('->', ' ').replace("\'", ' ')
            parts = line.split()  # tokenize call graph line
            graph_name = parts[0]
            parts = parts[1:]
            graph_names.append(graph_name)
            graph_functions = {}
            print(parts)

            for func in parts:
                if func not in defined_apis:  # ignore these API functions, they have already been counted.
                    if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):
                        func = func[:5]  # lets try to reduce the vast number of functions.
                    elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith(
                            'ecx+') or func.startswith('edx+'):
                        func = func[:5]
                    elif func.startswith('edi+') or func.startswith('esi+'):
                        func = func[:5]
                    elif func.startswith('byte_') or func.startswith('word_'):  # or func.startswith('nullsub')
                        func = func[:6]
                    else:  # reduce the feature set some more so my pissy pants PC can handle it.
                        func = func[:8]
                    if func not in column_names:  # NOTE: or in Defined APIs, these have already been counted.
                        column_names.append(func)

            counter += 1
            # Print progress
            if ((counter + 1) % 1000) == 0:
                print("Processed number {:d} Graph_name {:s} Total column names {:d}".format(counter, graph_name,
                                                                                             len(column_names)))

    with open(column_names_file, 'w') as cols:
        fw = writer(cols)
        fw.writerow(column_names)

    print("Completed writing {:d} column names.".format(len(column_names)))

    with open(graph_names_file, 'w') as gras:
        fw = writer(gras)
        fw.writerow(graph_names)

    print("Completed writing {:d} graph names.".format(len(graph_names)))


def batch_gen_column_names():
    gv_files = [f for f in os.listdir(DATA_OUT_DIR) if f.endswith("-malware-call-graphs.gv")]

    p = Pool(len(gv_files))
    p.map(generate_column_names, gv_files)

    print("gv file analysis done")

    counter = 0
    column_names = []
    column_name_files = [f for f in os.listdir(DATA_OUT_DIR) if f.endswith("-reduced-column-names.csv")]

    for cnamefile in column_name_files:
        with open(os.path.join(DATA_OUT_DIR, cnamefile), 'r') as cras:
            print("Starting file: {:s}".format(cnamefile))
            colstr = cras.readline()
            colnames = colstr.split(',')
            for cname in colnames:
                if cname not in column_names:
                    column_names.append(cname)

                counter += 1
                # Print progress
                if ((counter + 1) % 1000) == 0:
                    print("Processed column names {:d}".format(counter))

    with open(os.path.join(DATA_OUT_DIR, 'all-reduced-function-column-names.csv'), 'w') as cols:
        fw = writer(cols)
        fw.writerow(column_names)

    print("Completed writing column names total = {:d}".format(len(column_names)))


def generate_function_counts(call_graph_file):
    counter = 0
    error_count = 0

    fapi = open(os.path.join(BASE_DIR, "APIs.txt"))
    defined_apis = fapi.readlines()
    defined_apis = defined_apis[0].split(',')
    fapi.close()

    colf = open(os.path.join(DATA_OUT_DIR, 'all-reduced-function-column-names.csv'), 'r')
    all_column_names = []
    column_lines = colf.readlines()
    for line in column_lines:
        all_column_names += line.split(',')
    col_names_len = len(all_column_names)
    colf.close()
    print("Column Names: {:d}".format(col_names_len))

    pid = os.getpid()
    print('Process id:', pid)
    feature_file_name = os.path.join(DATA_OUT_DIR, str(pid) + '-call-graph-reduced-function_counts.csv')
    print('Call graph function counts file: {:s}'.format(feature_file_name))
    feature_file = open(feature_file_name, 'w')
    fw = writer(feature_file)

    call_graph_function_features = []

    with open(os.path.join(DATA_OUT_DIR, call_graph_file), 'r', errors='ignore') as cfg:
        for line in cfg:
            line.rstrip('\r\n')  # get rid of newlines they are annoying.
            # get rid of all these things they are annoying.
            line = line.replace(',', ' ').replace('[', ' ').replace(']', ' ').replace('->', ' ').replace("\'", ' ')
            parts = line.split()  # tokenize graph line

            graph_name = parts[0]
            parts = parts[1:]
            function_dict = {}

            # now generate the function counts for this call graph

            for func in parts:
                if func not in defined_apis:  # ignore these API functions, they have already been counted.
                    if func.startswith('sub') or func.startswith('loc') or func.startswith('unk'):
                        func = func[:5]  # lets try to reduce the vast number of functions.
                    elif func.startswith('eax+') or func.startswith('ebx+') or func.startswith(
                            'ecx+') or func.startswith('edx+'):
                        func = func[:5]
                    elif func.startswith('edi+') or func.startswith('esi+'):
                        func = func[:5]
                    elif func.startswith('byte_') or func.startswith('word_'):  # or func.startswith('nullsub')
                        func = func[:6]
                    else:  # reduce the feature set some more so my pissy pants PC can handle it.
                        func = func[:8]

                    if (func in function_dict):
                        function_dict[func] += 1
                    else:
                        function_dict[func] = 1

            # now generate the output row for this call graph

            function_counts = [0] * col_names_len  # zero everything because this is a sparse matrix
            for func in function_dict:
                for idx, cname in enumerate(all_column_names):
                    if func == cname:
                        function_counts[idx] = function_dict[func]
                        break

            call_graph_function_features.append([graph_name] + function_counts)

            # Print progress and write out rows
            counter += 1
            if ((counter + 1) % 100) == 0:
                print("{:d} Graph: {:s} Count: {:d}".format(pid, graph_name, counter))
                fw.writerows(call_graph_function_features)
                call_graph_function_features = []

        # Write remaining files
        if len(call_graph_function_features) > 0:
            fw.writerows(call_graph_function_features)
            call_graph_function_features = []

    feature_file.close()

    print("Completed processing {:d} graphs.".format(counter))


def batch_gen_function_counts():
    gv_files = [f for f in os.listdir(DATA_OUT_DIR) if f.endswith("-malware-call-graphs.gv")]

    p = Pool(len(gv_files))
    p.map(generate_function_counts, gv_files)


def concact_function_counts():
    rel_files = [f for f in os.listdir(DATA_OUT_DIR) if f.endswith("-call-graph-reduced-function_counts.csv")]

    df: pd.DataFrame = None

    # get head
    # columns_df = pd.read_csv(os.path.join(DATA_OUT_DIR, "all-reduced-function-column-names.csv"))
    # print(columns_df.columns)

    fw = open(os.path.join(DATA_OUT_DIR, "all-call-graph-reduced-function_counts.csv"), 'w')

    for rel_file in rel_files:
        print("processing ", rel_file)
        with open(os.path.join(DATA_OUT_DIR, rel_file), 'r') as fr:
            fw.write(fr.read())

    fw.close()


if __name__ == '__main__':
    # ExtraTreesClassifier()
    # concact_result()
    # batch_gen_column_names()
    # batch_gen_function_counts()
    concact_function_counts()
    pass
