import os
import numpy as np
import pandas as pd

from sklearn.metrics import log_loss, confusion_matrix, accuracy_score, classification_report, precision_score, \
    recall_score
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.model_selection import KFold

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier

from sklearn.preprocessing import StandardScaler

import xgboost as xgb
import lightgbm as lgb
from lightgbm.sklearn import LGBMClassifier

BASE_DIR = "/media/xueshan/WD_BLACK/cybersecurity/malaware"
# BASE_DIR = "/home/xueshan/temp/malaware"
TRAIN_DIR = os.path.join(BASE_DIR, "train")
TEST_DIR = os.path.join(BASE_DIR, "test")

DATA_OUT_DIR = os.path.join(BASE_DIR, "data_out")

TRAIN_FEAFURE_ASM = os.path.join(DATA_OUT_DIR, "train-malware-features-asm-30percent.csv")
TRAIN_FEAFURE_CALL = os.path.join(DATA_OUT_DIR, "train-call-graph-features-10percent.csv")
TRAIN_LABEL = os.path.join(DATA_OUT_DIR, "train-labels.csv")

TEST_FEATURE_ASM = os.path.join(DATA_OUT_DIR, "test-malware-features-asm-30percent.csv")
TEST_FEATURE_CALL = os.path.join(DATA_OUT_DIR, "test-call-graph-features-10percent.csv")
TEST_LABEL = os.path.join(DATA_OUT_DIR, "test-labels.csv")

RELEASE_MODE = False


def run_cv(X, y, clf):
    # Construct a kfolds object
    kf = KFold(10 if RELEASE_MODE else 3, shuffle=True)
    y_prob = np.zeros((len(y), 9))
    y_pred = np.zeros(len(y))

    # Iterate through folds
    for idx, (train_index, test_index) in enumerate(kf.split(X)):
        print("iter:", idx + 1, "train len:", len(train_index), "test len:", len(test_index))
        X_train = X.loc[train_index, :]
        X_test = X.loc[test_index, :]
        y_train = y[train_index]

        clf.fit(X_train, y_train)
        y_prob[test_index] = clf.predict_proba(X_test)
        y_pred[test_index] = clf.predict(X_test)

    return y_prob, y_pred


def normalize_feature(df_train, df_test):
    """
    normalize data to the data which mean is 0 and stdev is 1
    :param df_train:
    :param df_test:
    :return:
    """
    std = StandardScaler()
    for i in range(df_train.shape[1]):
        #  Use the mean and std of train data to transfrom test data
        std.fit(df_train.iloc[:, i].values.reshape(-1, 1))
        df_train.iloc[:, i] = std.transform(df_train.iloc[:, i].values.reshape(-1, 1)).flatten()
        df_test.iloc[:, i] = std.transform(df_test.iloc[:, i].values.reshape(-1, 1)).flatten()


def get_data_set():
    # read train
    df_train_asm = pd.read_csv(TRAIN_FEAFURE_ASM)
    df_train_call = pd.read_csv(TRAIN_FEAFURE_CALL)
    df_train_label = pd.read_csv(TRAIN_LABEL)

    df_test_asm = pd.read_csv(TEST_FEATURE_ASM)
    df_test_call = pd.read_csv(TEST_FEATURE_CALL)
    df_test_label = pd.read_csv(TEST_LABEL)

    assert np.all(df_test_asm.index == df_test_call.index) and np.all(df_train_call.index == df_train_label.index)

    df_train_features = pd.concat([df_train_asm.iloc[:, 1:], df_train_call.iloc[:, 1:]], axis=1)
    df_test_features = pd.concat([df_test_asm.iloc[:, 1:], df_test_call.iloc[:, 1:]], axis=1)

    print(df_train_features.head())
    print(df_train_features.describe())

    normalize_feature(df_train_features, df_test_features)
    print("after normalization:")

    print(df_train_features.head())
    print(df_train_features.describe())

    print("train features:", df_train_features.shape)
    print("test features:", df_test_features.shape)
    print("exception data number:", df_train_features.isnull().sum().sum())

    X_train = df_train_features
    y_train = df_train_label.iloc[:, 1].values
    X_test = df_test_features
    y_test = df_test_label.iloc[:, 1].values

    return X_train, y_train, X_test, y_test


def exe_ExtraTreesClassifier():
    X_train, y_train, X_test, y_test = get_data_set()

    # exit(0)

    # applying models

    # 基于树的分类器是不需要对特征进行标准化的
    # class_weight="balanced" 可以处理类别的样本量不平衡的问题
    clf1: ExtraTreesClassifier = ExtraTreesClassifier(n_estimators=1000, max_features=None, min_samples_leaf=1,
                                                      min_samples_split=9, n_jobs=14,
                                                      class_weight="balanced",
                                                      criterion='gini')
    p1, pred1 = run_cv(X_train, y_train, clf1)
    print("logloss = {:.4f}".format(log_loss(y_train, p1)))
    print("Accuracy: {:.4f}".format(accuracy_score(y_train, pred1)))
    print("Precision: {:.4f}".format(precision_score(y_train, pred1, average='weighted')))
    print("Recall: {:.4f}".format(recall_score(y_train, pred1, average='weighted')))
    cm = confusion_matrix(y_train, pred1)
    print(cm)

    print("test set:")

    # print result in test set
    test_pred = clf1.predict(X_test)
    print("Test accuracy: {:.4f}".format(accuracy_score(y_test, test_pred)))
    print("Test precision: {:.4f}".format(precision_score(y_test, test_pred, average='weighted')))
    print("Test recall: {:.4f}".format(recall_score(y_test, test_pred, average='weighted')))
    test_cm = confusion_matrix(y_test, test_pred)
    print(test_cm)


def sklearn_model_selection():
    names = ["Nearest Neighbors",
             # "Linear SVM",
             # "RBF SVM",
             "Decision Tree",
             "Random Forest",
             "AdaBoost",
             "Naive Bayes",
             # "Linear Discriminant Analysis",
             # "Quadratic Discriminant Analysis",
             "Extra Trees"]

    classifiers = [
        KNeighborsClassifier(),
        # SVC(kernel="linear", C=0.025, probability=True),
        # SVC(gamma=2, C=1, probability=True),
        DecisionTreeClassifier(max_depth=5),
        RandomForestClassifier(max_depth=5, n_estimators=1000),
        AdaBoostClassifier(),
        GaussianNB(),
        # LinearDiscriminantAnalysis(),
        # QuadraticDiscriminantAnalysis(),
        ExtraTreesClassifier(n_estimators=1000)]

    X_train, y_train, X_test, y_test = get_data_set()

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        print(name)
        prob, pred = run_cv(X_train, y_train, clf)
        # print("logloss: %.3f" % log_loss(y_train, prob))
        # print("train score = {:.4f}".format(accuracy_score(y_train, pred)))
        # cm = confusion_matrix(y_train, pred)
        # print(cm)

        print("test set:")

        # print result in test set
        test_pred = clf.predict(X_test)
        print("Test accuracy: {:.4f}".format(accuracy_score(y_test, test_pred)))
        print("Test precision: {:.4f}".format(precision_score(y_test, test_pred, average='weighted')))
        print("Test recall: {:.4f}".format(recall_score(y_test, test_pred, average='weighted')))
        test_cm = confusion_matrix(y_test, test_pred)
        print(test_cm)
        print("-" * 30)


def exe_xgboost():
    X_train, y_train, X_test, y_test = get_data_set()

    cols = X_train.columns.values
    rel = {}
    for col in cols:
        if col not in rel:
            rel[col] = 1
        else:
            rel[col] += 1
            print(col, ":", rel[col])

    new_cols = range(len(cols))
    X_train.columns = new_cols
    X_test.columns = new_cols

    assert len(X_train.columns) == len(set(X_train.columns))

    xgclf = xgb.XGBClassifier(n_estimators=1000, objective="multi:softmax", tree_method='gpu_hist', gpu_id=0)
    prob, pred = run_cv(X_train, y_train, xgclf)
    print("logloss: {:.4f}".format(log_loss(y_train, prob)))
    print("Accuracy: {:.4f}".format(accuracy_score(y_train, pred)))
    print("Precision: {:.4f}".format(precision_score(y_train, pred, average='weighted')))
    print("Recall: {:.4f}".format(recall_score(y_train, pred, average='weighted')))
    cm = confusion_matrix(y_train, pred)
    print(cm)

    print("test set:")

    # print result in test set
    test_pred = xgclf.predict(X_test)
    print("Test accuracy: {:.4f}".format(accuracy_score(y_test, test_pred)))
    print("Test precision: {:.4f}".format(precision_score(y_test, test_pred, average='weighted')))
    print("Test recall: {:.4f}".format(recall_score(y_test, test_pred, average='weighted')))
    test_cm = confusion_matrix(y_test, test_pred)
    print(test_cm)
    print("-" * 30)


def exe_lightgbm():
    X_train, y_train, X_test, y_test = get_data_set()

    cols = X_train.columns.values
    rel = {}
    for col in cols:
        if col not in rel:
            rel[col] = 1
        else:
            rel[col] += 1
            print(col, ":", rel[col])

    new_cols = range(len(cols))
    X_train.columns = new_cols
    X_test.columns = new_cols

    assert len(X_train.columns) == len(set(X_train.columns))

    gbm = LGBMClassifier("dart", n_estimators=1000, class_weight="balanced", device='gpu', gpu_use_dp=False)
    prob, pred = run_cv(X_train, y_train, gbm)
    print("logloss: {:.4f}".format(log_loss(y_train, prob)))
    print("Accuracy: {:.4f}".format(accuracy_score(y_train, pred)))
    print("Precision: {:.4f}".format(precision_score(y_train, pred, average='weighted')))
    print("Recall: {:.4f}".format(recall_score(y_train, pred, average='weighted')))
    cm = confusion_matrix(y_train, pred)
    print(cm)

    print("test set:")

    # print result in test set
    test_pred = gbm.predict(X_test)
    print("Test accuracy: {:.4f}".format(accuracy_score(y_test, test_pred)))
    print("Test precision: {:.4f}".format(precision_score(y_test, test_pred, average='weighted')))
    print("Test recall: {:.4f}".format(recall_score(y_test, test_pred, average='weighted')))
    test_cm = confusion_matrix(y_test, test_pred)
    print(test_cm)
    print("-" * 30)


if __name__ == '__main__':
    # sklearn_model_selection()
    # exe_ExtraTreesClassifier()
    exe_xgboost()
    exe_lightgbm()
